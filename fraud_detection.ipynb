{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9de27a1-4fa9-41a2-8d05-5f09ab6047b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/09/13 18:03:09 WARN Utils: Your hostname, Arshdeeps-MacBook-Pro.local, resolves to a loopback address: 127.0.0.1; using 192.168.1.161 instead (on interface en0)\n",
      "25/09/13 18:03:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/13 18:03:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set JAVA_HOME to the correct Java 17 path\n",
    "os.environ[\"JAVA_HOME\"] = \"/opt/homebrew/Cellar/openjdk@17/17.0.16/libexec/openjdk.jdk/Contents/Home\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "\n",
    "# Then import PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FraudDetection\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a5d71c7-91b0-4d67-9fb3-8dd5a348782f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- step: integer (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- nameOrig: string (nullable = true)\n",
      " |-- oldbalanceOrg: double (nullable = true)\n",
      " |-- newbalanceOrig: double (nullable = true)\n",
      " |-- nameDest: string (nullable = true)\n",
      " |-- oldbalanceDest: double (nullable = true)\n",
      " |-- newbalanceDest: double (nullable = true)\n",
      " |-- isFraud: integer (nullable = true)\n",
      " |-- isFlaggedFraud: integer (nullable = true)\n",
      "\n",
      "+----+--------+--------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
      "|step|    type|  amount|   nameOrig|oldbalanceOrg|newbalanceOrig|   nameDest|oldbalanceDest|newbalanceDest|isFraud|isFlaggedFraud|\n",
      "+----+--------+--------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
      "|   1| PAYMENT| 9839.64|C1231006815|     170136.0|     160296.36|M1979787155|           0.0|           0.0|      0|             0|\n",
      "|   1| PAYMENT| 1864.28|C1666544295|      21249.0|      19384.72|M2044282225|           0.0|           0.0|      0|             0|\n",
      "|   1|TRANSFER|   181.0|C1305486145|        181.0|           0.0| C553264065|           0.0|           0.0|      1|             0|\n",
      "|   1|CASH_OUT|   181.0| C840083671|        181.0|           0.0|  C38997010|       21182.0|           0.0|      1|             0|\n",
      "|   1| PAYMENT|11668.14|C2048537720|      41554.0|      29885.86|M1230701703|           0.0|           0.0|      0|             0|\n",
      "+----+--------+--------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.csv(\"credit_card.csv\", header=True, inferSchema=True)\n",
    "data.printSchema()\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fcd0d2-352e-494e-b2b5-88a15acfe065",
   "metadata": {},
   "source": [
    "### Dataset Column Descriptions\n",
    "\n",
    "- **step**: Time step of the transaction.\n",
    "- **type**: Type of transaction (PAYMENT, TRANSFER, CASH_OUT, etc.).\n",
    "- **amount**: Amount of money involved in the transaction.\n",
    "- **nameOrig**: Account ID of the transaction originator.\n",
    "- **oldbalanceOrg**: Balance of the originator before the transaction.\n",
    "- **newbalanceOrig**: Balance of the originator after the transaction.\n",
    "- **nameDest**: Account ID of the transaction receiver.\n",
    "- **oldbalanceDest**: Balance of the destination account before the transaction.\n",
    "- **newbalanceDest**: Balance of the destination account after the transaction.\n",
    "- **isFraud**: 1 if the transaction is fraudulent, 0 otherwise (target variable).\n",
    "- **isFlaggedFraud**: 1 if the system flagged the transaction as fraud, 0 otherwise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8adab712-429b-409b-bb66-bfaea8948b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|isFraud|  count|\n",
      "+-------+-------+\n",
      "|      1|   8213|\n",
      "|      0|6354407|\n",
      "+-------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "data.groupBy(\"isFraud\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c77a88-8a68-4979-991f-0a8f3cbcca76",
   "metadata": {},
   "source": [
    "Data is Highly Imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19d02f5b-794b-4cec-a90d-b76dbc57f311",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                        (0 + 10) / 10]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------+--------+-------------+--------------+--------+--------------+--------------+-------+--------------+\n",
      "|step|type|amount|nameOrig|oldbalanceOrg|newbalanceOrig|nameDest|oldbalanceDest|newbalanceDest|isFraud|isFlaggedFraud|\n",
      "+----+----+------+--------+-------------+--------------+--------+--------------+--------------+-------+--------------+\n",
      "|   0|   0|     0|       0|            0|             0|       0|             0|             0|      0|             0|\n",
      "+----+----+------+--------+-------------+--------------+--------+--------------+--------------+-------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "data.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in data.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb285314-ff1d-47dc-a2ee-9ea8cb97beea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|isFraud|      classWeightCol|\n",
      "+-------+--------------------+\n",
      "|      0|0.001290820448180152|\n",
      "|      0|0.001290820448180152|\n",
      "|      1|  0.9987091795518198|\n",
      "|      1|  0.9987091795518198|\n",
      "|      0|0.001290820448180152|\n",
      "+-------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Total counts\n",
    "total = data.count()\n",
    "fraud_count = data.filter(col(\"isFraud\") == 1).count()\n",
    "normal_count = data.filter(col(\"isFraud\") == 0).count()\n",
    "\n",
    "# Assign weight\n",
    "data = data.withColumn(\n",
    "    \"classWeightCol\",\n",
    "    when(col(\"isFraud\") == 1, normal_count / total)\n",
    "    .otherwise(fraud_count / total)\n",
    ")\n",
    "data.select(\"isFraud\", \"classWeightCol\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27b31f29-d623-4618-ac2a-990909db4d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------------------+\n",
      "|            features|isFraud|      classWeightCol|\n",
      "+--------------------+-------+--------------------+\n",
      "|[1.0,9839.64,1701...|      0|0.001290820448180152|\n",
      "|[1.0,1864.28,2124...|      0|0.001290820448180152|\n",
      "|[1.0,181.0,181.0,...|      1|  0.9987091795518198|\n",
      "|(9,[0,1,2,4],[1.0...|      1|  0.9987091795518198|\n",
      "|[1.0,11668.14,415...|      0|0.001290820448180152|\n",
      "+--------------------+-------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "\n",
    "# Convert 'type' column to numeric\n",
    "indexer = StringIndexer(inputCol=\"type\", outputCol=\"typeIndex\")\n",
    "data = indexer.fit(data).transform(data)\n",
    "\n",
    "# Select numeric columns only\n",
    "feature_cols = [c for c in data.columns if c not in (\"isFraud\", \"classWeightCol\", \"nameOrig\", \"nameDest\", \"type\")]\n",
    "\n",
    "# Include the encoded 'typeIndex'\n",
    "feature_cols.append(\"typeIndex\")\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "final_data = assembler.transform(data).select(\"features\", \"isFraud\", \"classWeightCol\")\n",
    "final_data.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94b0cf94-3563-4435-b248-b04e2e295148",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = final_data.randomSplit([0.8, 0.2], seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d295bbb8-9ea9-4beb-aceb-26e2112236c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/13 18:03:25 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/09/13 18:03:28 WARN MemoryStore: Not enough space to cache rdd_87_7 in memory! (computed 17.0 MiB so far)\n",
      "25/09/13 18:03:28 WARN BlockManager: Persisting block rdd_87_7 to disk instead.\n",
      "25/09/13 18:03:28 WARN MemoryStore: Not enough space to cache rdd_87_3 in memory! (computed 17.0 MiB so far)\n",
      "25/09/13 18:03:28 WARN BlockManager: Persisting block rdd_87_3 to disk instead.\n",
      "25/09/13 18:03:28 WARN MemoryStore: Not enough space to cache rdd_87_2 in memory! (computed 17.0 MiB so far)\n",
      "25/09/13 18:03:28 WARN BlockManager: Persisting block rdd_87_2 to disk instead.\n",
      "25/09/13 18:03:28 WARN MemoryStore: Not enough space to cache rdd_87_8 in memory! (computed 17.0 MiB so far)\n",
      "25/09/13 18:03:28 WARN BlockManager: Persisting block rdd_87_8 to disk instead.\n",
      "25/09/13 18:03:28 WARN MemoryStore: Not enough space to cache rdd_87_6 in memory! (computed 17.0 MiB so far)\n",
      "25/09/13 18:03:28 WARN BlockManager: Persisting block rdd_87_6 to disk instead.\n",
      "25/09/13 18:03:28 WARN MemoryStore: Not enough space to cache rdd_87_0 in memory! (computed 17.0 MiB so far)\n",
      "25/09/13 18:03:28 WARN BlockManager: Persisting block rdd_87_0 to disk instead.\n",
      "25/09/13 18:03:28 WARN MemoryStore: Not enough space to cache rdd_87_9 in memory! (computed 33.0 MiB so far)\n",
      "25/09/13 18:03:28 WARN BlockManager: Persisting block rdd_87_9 to disk instead.\n",
      "25/09/13 18:03:28 WARN MemoryStore: Not enough space to cache rdd_87_1 in memory! (computed 17.0 MiB so far)\n",
      "25/09/13 18:03:28 WARN BlockManager: Persisting block rdd_87_1 to disk instead.\n",
      "25/09/13 18:03:28 WARN MemoryStore: Not enough space to cache rdd_87_5 in memory! (computed 17.0 MiB so far)\n",
      "25/09/13 18:03:28 WARN BlockManager: Persisting block rdd_87_5 to disk instead.\n",
      "25/09/13 18:03:29 WARN MemoryStore: Not enough space to cache rdd_87_9 in memory! (computed 33.0 MiB so far)\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(\n",
    "    labelCol=\"isFraud\",\n",
    "    featuresCol=\"features\",\n",
    "    weightCol=\"classWeightCol\"  # important for imbalance\n",
    ")\n",
    "\n",
    "model = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f9ff289-2fa6-4896-aea1-4a63602c101f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9833267974553004\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "predictions = model.transform(test)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"isFraud\")\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(\"AUC:\", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dbfd47e-1231-499a-b9a4-10b147f29100",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 205:===========>                                            (2 + 8) / 10]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.9833641556525715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"isFraud\", predictionCol=\"prediction\", metricName=\"f1\"\n",
    ")\n",
    "f1 = evaluator.evaluate(predictions)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4195e76a-7e89-4258-a7dd-38580acb2c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"fraud_detection_model_spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0c2870-d7f7-4457-ba1f-3e77f8a08ebd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Java 17)",
   "language": "python",
   "name": "pyspark17"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
